{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 1.6318367346938776,
  "eval_steps": 500,
  "global_step": 1000,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0163265306122449,
      "grad_norm": 0.6070590615272522,
      "learning_rate": 0.00019902120717781404,
      "loss": 2.5487,
      "step": 10
    },
    {
      "epoch": 0.0326530612244898,
      "grad_norm": 1.049483060836792,
      "learning_rate": 0.00019793365959760743,
      "loss": 2.064,
      "step": 20
    },
    {
      "epoch": 0.04897959183673469,
      "grad_norm": 0.3615463972091675,
      "learning_rate": 0.00019684611201740079,
      "loss": 1.9007,
      "step": 30
    },
    {
      "epoch": 0.0653061224489796,
      "grad_norm": 0.2845410108566284,
      "learning_rate": 0.00019575856443719415,
      "loss": 1.8886,
      "step": 40
    },
    {
      "epoch": 0.08163265306122448,
      "grad_norm": 0.3410886526107788,
      "learning_rate": 0.0001946710168569875,
      "loss": 1.8426,
      "step": 50
    },
    {
      "epoch": 0.09795918367346938,
      "grad_norm": 0.30703961849212646,
      "learning_rate": 0.00019358346927678087,
      "loss": 1.8371,
      "step": 60
    },
    {
      "epoch": 0.11428571428571428,
      "grad_norm": 0.31881651282310486,
      "learning_rate": 0.00019249592169657425,
      "loss": 1.9293,
      "step": 70
    },
    {
      "epoch": 0.1306122448979592,
      "grad_norm": 0.2803342938423157,
      "learning_rate": 0.0001914083741163676,
      "loss": 1.818,
      "step": 80
    },
    {
      "epoch": 0.1469387755102041,
      "grad_norm": 0.37725797295570374,
      "learning_rate": 0.00019032082653616097,
      "loss": 1.8202,
      "step": 90
    },
    {
      "epoch": 0.16326530612244897,
      "grad_norm": 0.35854172706604004,
      "learning_rate": 0.00018923327895595433,
      "loss": 1.8213,
      "step": 100
    },
    {
      "epoch": 0.17959183673469387,
      "grad_norm": 0.3202967047691345,
      "learning_rate": 0.0001881457313757477,
      "loss": 1.8243,
      "step": 110
    },
    {
      "epoch": 0.19591836734693877,
      "grad_norm": 0.3223319351673126,
      "learning_rate": 0.00018705818379554105,
      "loss": 1.8553,
      "step": 120
    },
    {
      "epoch": 0.21224489795918366,
      "grad_norm": 0.3044050335884094,
      "learning_rate": 0.00018597063621533444,
      "loss": 1.804,
      "step": 130
    },
    {
      "epoch": 0.22857142857142856,
      "grad_norm": 0.2949411869049072,
      "learning_rate": 0.0001848830886351278,
      "loss": 1.8266,
      "step": 140
    },
    {
      "epoch": 0.24489795918367346,
      "grad_norm": 0.31954482197761536,
      "learning_rate": 0.00018379554105492116,
      "loss": 1.7876,
      "step": 150
    },
    {
      "epoch": 0.2612244897959184,
      "grad_norm": 0.32863157987594604,
      "learning_rate": 0.00018270799347471452,
      "loss": 1.8,
      "step": 160
    },
    {
      "epoch": 0.27755102040816326,
      "grad_norm": 0.3236066997051239,
      "learning_rate": 0.00018162044589450788,
      "loss": 1.8107,
      "step": 170
    },
    {
      "epoch": 0.2938775510204082,
      "grad_norm": 0.31270483136177063,
      "learning_rate": 0.00018053289831430127,
      "loss": 1.8301,
      "step": 180
    },
    {
      "epoch": 0.31020408163265306,
      "grad_norm": 0.2875197231769562,
      "learning_rate": 0.00017944535073409463,
      "loss": 1.7883,
      "step": 190
    },
    {
      "epoch": 0.32653061224489793,
      "grad_norm": 0.3045722544193268,
      "learning_rate": 0.000178357803153888,
      "loss": 1.726,
      "step": 200
    },
    {
      "epoch": 0.34285714285714286,
      "grad_norm": 0.42726588249206543,
      "learning_rate": 0.00017727025557368135,
      "loss": 1.7654,
      "step": 210
    },
    {
      "epoch": 0.35918367346938773,
      "grad_norm": 0.3126099407672882,
      "learning_rate": 0.0001761827079934747,
      "loss": 1.7919,
      "step": 220
    },
    {
      "epoch": 0.37551020408163266,
      "grad_norm": 0.3070201873779297,
      "learning_rate": 0.00017509516041326807,
      "loss": 1.8171,
      "step": 230
    },
    {
      "epoch": 0.39183673469387753,
      "grad_norm": 0.31151145696640015,
      "learning_rate": 0.00017400761283306145,
      "loss": 1.7921,
      "step": 240
    },
    {
      "epoch": 0.40816326530612246,
      "grad_norm": 0.3321877419948578,
      "learning_rate": 0.00017292006525285481,
      "loss": 1.7703,
      "step": 250
    },
    {
      "epoch": 0.42448979591836733,
      "grad_norm": 0.37979647517204285,
      "learning_rate": 0.00017183251767264817,
      "loss": 1.8107,
      "step": 260
    },
    {
      "epoch": 0.44081632653061226,
      "grad_norm": 0.4087633192539215,
      "learning_rate": 0.00017074497009244153,
      "loss": 1.7817,
      "step": 270
    },
    {
      "epoch": 0.45714285714285713,
      "grad_norm": 0.3711621165275574,
      "learning_rate": 0.00016965742251223492,
      "loss": 1.7706,
      "step": 280
    },
    {
      "epoch": 0.47346938775510206,
      "grad_norm": 0.34205716848373413,
      "learning_rate": 0.00016856987493202828,
      "loss": 1.7466,
      "step": 290
    },
    {
      "epoch": 0.4897959183673469,
      "grad_norm": 0.3816305994987488,
      "learning_rate": 0.00016748232735182167,
      "loss": 1.7623,
      "step": 300
    },
    {
      "epoch": 0.5061224489795918,
      "grad_norm": 0.2984622120857239,
      "learning_rate": 0.00016639477977161503,
      "loss": 1.7816,
      "step": 310
    },
    {
      "epoch": 0.5224489795918368,
      "grad_norm": 0.33755743503570557,
      "learning_rate": 0.0001653072321914084,
      "loss": 1.73,
      "step": 320
    },
    {
      "epoch": 0.5387755102040817,
      "grad_norm": 0.28599584102630615,
      "learning_rate": 0.00016421968461120175,
      "loss": 1.7403,
      "step": 330
    },
    {
      "epoch": 0.5551020408163265,
      "grad_norm": 0.3225797712802887,
      "learning_rate": 0.00016313213703099514,
      "loss": 1.8141,
      "step": 340
    },
    {
      "epoch": 0.5714285714285714,
      "grad_norm": 0.34483781456947327,
      "learning_rate": 0.0001620445894507885,
      "loss": 1.7454,
      "step": 350
    },
    {
      "epoch": 0.5877551020408164,
      "grad_norm": 0.3487129211425781,
      "learning_rate": 0.00016095704187058186,
      "loss": 1.7941,
      "step": 360
    },
    {
      "epoch": 0.6040816326530613,
      "grad_norm": 0.33115166425704956,
      "learning_rate": 0.00015986949429037522,
      "loss": 1.8038,
      "step": 370
    },
    {
      "epoch": 0.6204081632653061,
      "grad_norm": 0.3212842643260956,
      "learning_rate": 0.00015878194671016858,
      "loss": 1.7753,
      "step": 380
    },
    {
      "epoch": 0.636734693877551,
      "grad_norm": 0.33445823192596436,
      "learning_rate": 0.00015769439912996194,
      "loss": 1.7847,
      "step": 390
    },
    {
      "epoch": 0.6530612244897959,
      "grad_norm": 0.3554426431655884,
      "learning_rate": 0.00015660685154975532,
      "loss": 1.747,
      "step": 400
    },
    {
      "epoch": 0.6693877551020408,
      "grad_norm": 0.391044557094574,
      "learning_rate": 0.00015551930396954868,
      "loss": 1.7959,
      "step": 410
    },
    {
      "epoch": 0.6857142857142857,
      "grad_norm": 0.3506777882575989,
      "learning_rate": 0.00015443175638934204,
      "loss": 1.7358,
      "step": 420
    },
    {
      "epoch": 0.7020408163265306,
      "grad_norm": 0.3637069761753082,
      "learning_rate": 0.0001533442088091354,
      "loss": 1.7738,
      "step": 430
    },
    {
      "epoch": 0.7183673469387755,
      "grad_norm": 0.32512950897216797,
      "learning_rate": 0.00015225666122892876,
      "loss": 1.7895,
      "step": 440
    },
    {
      "epoch": 0.7346938775510204,
      "grad_norm": 0.30562251806259155,
      "learning_rate": 0.00015116911364872215,
      "loss": 1.7917,
      "step": 450
    },
    {
      "epoch": 0.7510204081632653,
      "grad_norm": 0.31879860162734985,
      "learning_rate": 0.0001500815660685155,
      "loss": 1.7348,
      "step": 460
    },
    {
      "epoch": 0.7673469387755102,
      "grad_norm": 0.3439510464668274,
      "learning_rate": 0.00014899401848830887,
      "loss": 1.7827,
      "step": 470
    },
    {
      "epoch": 0.7836734693877551,
      "grad_norm": 0.37851470708847046,
      "learning_rate": 0.00014790647090810223,
      "loss": 1.7644,
      "step": 480
    },
    {
      "epoch": 0.8,
      "grad_norm": 0.35986047983169556,
      "learning_rate": 0.0001468189233278956,
      "loss": 1.7609,
      "step": 490
    },
    {
      "epoch": 0.8163265306122449,
      "grad_norm": 0.3248812258243561,
      "learning_rate": 0.00014573137574768895,
      "loss": 1.7945,
      "step": 500
    },
    {
      "epoch": 0.8326530612244898,
      "grad_norm": 0.39814943075180054,
      "learning_rate": 0.00014464382816748234,
      "loss": 1.8043,
      "step": 510
    },
    {
      "epoch": 0.8489795918367347,
      "grad_norm": 0.3865431845188141,
      "learning_rate": 0.0001435562805872757,
      "loss": 1.7499,
      "step": 520
    },
    {
      "epoch": 0.8653061224489796,
      "grad_norm": 0.3350381553173065,
      "learning_rate": 0.00014246873300706906,
      "loss": 1.7892,
      "step": 530
    },
    {
      "epoch": 0.8816326530612245,
      "grad_norm": 0.32461676001548767,
      "learning_rate": 0.00014138118542686242,
      "loss": 1.8313,
      "step": 540
    },
    {
      "epoch": 0.8979591836734694,
      "grad_norm": 0.30304235219955444,
      "learning_rate": 0.0001402936378466558,
      "loss": 1.7462,
      "step": 550
    },
    {
      "epoch": 0.9142857142857143,
      "grad_norm": 0.31604698300361633,
      "learning_rate": 0.00013920609026644916,
      "loss": 1.8396,
      "step": 560
    },
    {
      "epoch": 0.9306122448979591,
      "grad_norm": 0.35910564661026,
      "learning_rate": 0.00013811854268624252,
      "loss": 1.7799,
      "step": 570
    },
    {
      "epoch": 0.9469387755102041,
      "grad_norm": 0.37859055399894714,
      "learning_rate": 0.0001370309951060359,
      "loss": 1.7635,
      "step": 580
    },
    {
      "epoch": 0.963265306122449,
      "grad_norm": 0.35883405804634094,
      "learning_rate": 0.00013594344752582927,
      "loss": 1.739,
      "step": 590
    },
    {
      "epoch": 0.9795918367346939,
      "grad_norm": 0.3620239198207855,
      "learning_rate": 0.00013485589994562263,
      "loss": 1.64,
      "step": 600
    },
    {
      "epoch": 0.9959183673469387,
      "grad_norm": 0.3633042871952057,
      "learning_rate": 0.000133768352365416,
      "loss": 1.7418,
      "step": 610
    },
    {
      "epoch": 1.0114285714285713,
      "grad_norm": 0.30214953422546387,
      "learning_rate": 0.00013268080478520938,
      "loss": 1.7592,
      "step": 620
    },
    {
      "epoch": 1.0277551020408162,
      "grad_norm": 0.34190449118614197,
      "learning_rate": 0.00013159325720500274,
      "loss": 1.7388,
      "step": 630
    },
    {
      "epoch": 1.0440816326530613,
      "grad_norm": 0.3426276743412018,
      "learning_rate": 0.0001305057096247961,
      "loss": 1.6961,
      "step": 640
    },
    {
      "epoch": 1.0604081632653062,
      "grad_norm": 0.35828086733818054,
      "learning_rate": 0.00012941816204458946,
      "loss": 1.681,
      "step": 650
    },
    {
      "epoch": 1.076734693877551,
      "grad_norm": 0.3839339017868042,
      "learning_rate": 0.00012833061446438282,
      "loss": 1.7052,
      "step": 660
    },
    {
      "epoch": 1.093061224489796,
      "grad_norm": 0.41961991786956787,
      "learning_rate": 0.0001272430668841762,
      "loss": 1.7308,
      "step": 670
    },
    {
      "epoch": 1.1093877551020408,
      "grad_norm": 0.3190782070159912,
      "learning_rate": 0.00012615551930396956,
      "loss": 1.7539,
      "step": 680
    },
    {
      "epoch": 1.1257142857142857,
      "grad_norm": 0.39733657240867615,
      "learning_rate": 0.00012506797172376292,
      "loss": 1.808,
      "step": 690
    },
    {
      "epoch": 1.1420408163265305,
      "grad_norm": 0.419784277677536,
      "learning_rate": 0.00012398042414355628,
      "loss": 1.7209,
      "step": 700
    },
    {
      "epoch": 1.1583673469387756,
      "grad_norm": 0.4370533525943756,
      "learning_rate": 0.00012289287656334964,
      "loss": 1.7237,
      "step": 710
    },
    {
      "epoch": 1.1746938775510205,
      "grad_norm": 0.3911590874195099,
      "learning_rate": 0.000121805328983143,
      "loss": 1.6792,
      "step": 720
    },
    {
      "epoch": 1.1910204081632654,
      "grad_norm": 0.4202093183994293,
      "learning_rate": 0.00012071778140293639,
      "loss": 1.7578,
      "step": 730
    },
    {
      "epoch": 1.2073469387755102,
      "grad_norm": 0.4403725266456604,
      "learning_rate": 0.00011963023382272975,
      "loss": 1.6709,
      "step": 740
    },
    {
      "epoch": 1.2236734693877551,
      "grad_norm": 0.4282209277153015,
      "learning_rate": 0.00011854268624252311,
      "loss": 1.7659,
      "step": 750
    },
    {
      "epoch": 1.24,
      "grad_norm": 0.40182796120643616,
      "learning_rate": 0.00011745513866231647,
      "loss": 1.6835,
      "step": 760
    },
    {
      "epoch": 1.2563265306122449,
      "grad_norm": 0.42794710397720337,
      "learning_rate": 0.00011636759108210984,
      "loss": 1.6941,
      "step": 770
    },
    {
      "epoch": 1.2726530612244897,
      "grad_norm": 0.4272332489490509,
      "learning_rate": 0.00011528004350190322,
      "loss": 1.7426,
      "step": 780
    },
    {
      "epoch": 1.2889795918367346,
      "grad_norm": 0.4609212875366211,
      "learning_rate": 0.00011419249592169658,
      "loss": 1.7324,
      "step": 790
    },
    {
      "epoch": 1.3053061224489797,
      "grad_norm": 0.48161691427230835,
      "learning_rate": 0.00011310494834148995,
      "loss": 1.7568,
      "step": 800
    },
    {
      "epoch": 1.3216326530612246,
      "grad_norm": 0.3699030876159668,
      "learning_rate": 0.00011201740076128331,
      "loss": 1.6905,
      "step": 810
    },
    {
      "epoch": 1.3379591836734694,
      "grad_norm": 0.41153985261917114,
      "learning_rate": 0.00011092985318107667,
      "loss": 1.7356,
      "step": 820
    },
    {
      "epoch": 1.3542857142857143,
      "grad_norm": 0.4247668981552124,
      "learning_rate": 0.00010984230560087003,
      "loss": 1.7839,
      "step": 830
    },
    {
      "epoch": 1.3706122448979592,
      "grad_norm": 0.4074098467826843,
      "learning_rate": 0.00010875475802066342,
      "loss": 1.7578,
      "step": 840
    },
    {
      "epoch": 1.386938775510204,
      "grad_norm": 0.39793357253074646,
      "learning_rate": 0.00010766721044045678,
      "loss": 1.7218,
      "step": 850
    },
    {
      "epoch": 1.403265306122449,
      "grad_norm": 0.44854822754859924,
      "learning_rate": 0.00010657966286025014,
      "loss": 1.7176,
      "step": 860
    },
    {
      "epoch": 1.4195918367346938,
      "grad_norm": 0.3655751645565033,
      "learning_rate": 0.0001054921152800435,
      "loss": 1.7131,
      "step": 870
    },
    {
      "epoch": 1.4359183673469387,
      "grad_norm": 0.44687238335609436,
      "learning_rate": 0.00010440456769983686,
      "loss": 1.7208,
      "step": 880
    },
    {
      "epoch": 1.4522448979591838,
      "grad_norm": 0.4253818392753601,
      "learning_rate": 0.00010331702011963025,
      "loss": 1.7614,
      "step": 890
    },
    {
      "epoch": 1.4685714285714286,
      "grad_norm": 0.44327694177627563,
      "learning_rate": 0.0001022294725394236,
      "loss": 1.7722,
      "step": 900
    },
    {
      "epoch": 1.4848979591836735,
      "grad_norm": 0.4031260013580322,
      "learning_rate": 0.00010114192495921697,
      "loss": 1.7173,
      "step": 910
    },
    {
      "epoch": 1.5012244897959184,
      "grad_norm": 0.4226587116718292,
      "learning_rate": 0.00010005437737901034,
      "loss": 1.794,
      "step": 920
    },
    {
      "epoch": 1.5175510204081633,
      "grad_norm": 0.5013386011123657,
      "learning_rate": 9.896682979880371e-05,
      "loss": 1.7564,
      "step": 930
    },
    {
      "epoch": 1.5338775510204081,
      "grad_norm": 0.40163445472717285,
      "learning_rate": 9.787928221859707e-05,
      "loss": 1.7359,
      "step": 940
    },
    {
      "epoch": 1.550204081632653,
      "grad_norm": 0.4464458227157593,
      "learning_rate": 9.679173463839043e-05,
      "loss": 1.7888,
      "step": 950
    },
    {
      "epoch": 1.566530612244898,
      "grad_norm": 0.428595632314682,
      "learning_rate": 9.57041870581838e-05,
      "loss": 1.7221,
      "step": 960
    },
    {
      "epoch": 1.5828571428571427,
      "grad_norm": 0.46001940965652466,
      "learning_rate": 9.461663947797717e-05,
      "loss": 1.7238,
      "step": 970
    },
    {
      "epoch": 1.5991836734693878,
      "grad_norm": 0.4248057007789612,
      "learning_rate": 9.352909189777053e-05,
      "loss": 1.7003,
      "step": 980
    },
    {
      "epoch": 1.6155102040816327,
      "grad_norm": 0.3950120210647583,
      "learning_rate": 9.24415443175639e-05,
      "loss": 1.6982,
      "step": 990
    },
    {
      "epoch": 1.6318367346938776,
      "grad_norm": 0.4902910590171814,
      "learning_rate": 9.135399673735726e-05,
      "loss": 1.7592,
      "step": 1000
    }
  ],
  "logging_steps": 10,
  "max_steps": 1839,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 3.1907610733471334e+17,
  "train_batch_size": 2,
  "trial_name": null,
  "trial_params": null
}
