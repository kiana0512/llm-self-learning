{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 3.0,
  "eval_steps": 500,
  "global_step": 1839,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0163265306122449,
      "grad_norm": 0.6070590615272522,
      "learning_rate": 0.00019902120717781404,
      "loss": 2.5487,
      "step": 10
    },
    {
      "epoch": 0.0326530612244898,
      "grad_norm": 1.049483060836792,
      "learning_rate": 0.00019793365959760743,
      "loss": 2.064,
      "step": 20
    },
    {
      "epoch": 0.04897959183673469,
      "grad_norm": 0.3615463972091675,
      "learning_rate": 0.00019684611201740079,
      "loss": 1.9007,
      "step": 30
    },
    {
      "epoch": 0.0653061224489796,
      "grad_norm": 0.2845410108566284,
      "learning_rate": 0.00019575856443719415,
      "loss": 1.8886,
      "step": 40
    },
    {
      "epoch": 0.08163265306122448,
      "grad_norm": 0.3410886526107788,
      "learning_rate": 0.0001946710168569875,
      "loss": 1.8426,
      "step": 50
    },
    {
      "epoch": 0.09795918367346938,
      "grad_norm": 0.30703961849212646,
      "learning_rate": 0.00019358346927678087,
      "loss": 1.8371,
      "step": 60
    },
    {
      "epoch": 0.11428571428571428,
      "grad_norm": 0.31881651282310486,
      "learning_rate": 0.00019249592169657425,
      "loss": 1.9293,
      "step": 70
    },
    {
      "epoch": 0.1306122448979592,
      "grad_norm": 0.2803342938423157,
      "learning_rate": 0.0001914083741163676,
      "loss": 1.818,
      "step": 80
    },
    {
      "epoch": 0.1469387755102041,
      "grad_norm": 0.37725797295570374,
      "learning_rate": 0.00019032082653616097,
      "loss": 1.8202,
      "step": 90
    },
    {
      "epoch": 0.16326530612244897,
      "grad_norm": 0.35854172706604004,
      "learning_rate": 0.00018923327895595433,
      "loss": 1.8213,
      "step": 100
    },
    {
      "epoch": 0.17959183673469387,
      "grad_norm": 0.3202967047691345,
      "learning_rate": 0.0001881457313757477,
      "loss": 1.8243,
      "step": 110
    },
    {
      "epoch": 0.19591836734693877,
      "grad_norm": 0.3223319351673126,
      "learning_rate": 0.00018705818379554105,
      "loss": 1.8553,
      "step": 120
    },
    {
      "epoch": 0.21224489795918366,
      "grad_norm": 0.3044050335884094,
      "learning_rate": 0.00018597063621533444,
      "loss": 1.804,
      "step": 130
    },
    {
      "epoch": 0.22857142857142856,
      "grad_norm": 0.2949411869049072,
      "learning_rate": 0.0001848830886351278,
      "loss": 1.8266,
      "step": 140
    },
    {
      "epoch": 0.24489795918367346,
      "grad_norm": 0.31954482197761536,
      "learning_rate": 0.00018379554105492116,
      "loss": 1.7876,
      "step": 150
    },
    {
      "epoch": 0.2612244897959184,
      "grad_norm": 0.32863157987594604,
      "learning_rate": 0.00018270799347471452,
      "loss": 1.8,
      "step": 160
    },
    {
      "epoch": 0.27755102040816326,
      "grad_norm": 0.3236066997051239,
      "learning_rate": 0.00018162044589450788,
      "loss": 1.8107,
      "step": 170
    },
    {
      "epoch": 0.2938775510204082,
      "grad_norm": 0.31270483136177063,
      "learning_rate": 0.00018053289831430127,
      "loss": 1.8301,
      "step": 180
    },
    {
      "epoch": 0.31020408163265306,
      "grad_norm": 0.2875197231769562,
      "learning_rate": 0.00017944535073409463,
      "loss": 1.7883,
      "step": 190
    },
    {
      "epoch": 0.32653061224489793,
      "grad_norm": 0.3045722544193268,
      "learning_rate": 0.000178357803153888,
      "loss": 1.726,
      "step": 200
    },
    {
      "epoch": 0.34285714285714286,
      "grad_norm": 0.42726588249206543,
      "learning_rate": 0.00017727025557368135,
      "loss": 1.7654,
      "step": 210
    },
    {
      "epoch": 0.35918367346938773,
      "grad_norm": 0.3126099407672882,
      "learning_rate": 0.0001761827079934747,
      "loss": 1.7919,
      "step": 220
    },
    {
      "epoch": 0.37551020408163266,
      "grad_norm": 0.3070201873779297,
      "learning_rate": 0.00017509516041326807,
      "loss": 1.8171,
      "step": 230
    },
    {
      "epoch": 0.39183673469387753,
      "grad_norm": 0.31151145696640015,
      "learning_rate": 0.00017400761283306145,
      "loss": 1.7921,
      "step": 240
    },
    {
      "epoch": 0.40816326530612246,
      "grad_norm": 0.3321877419948578,
      "learning_rate": 0.00017292006525285481,
      "loss": 1.7703,
      "step": 250
    },
    {
      "epoch": 0.42448979591836733,
      "grad_norm": 0.37979647517204285,
      "learning_rate": 0.00017183251767264817,
      "loss": 1.8107,
      "step": 260
    },
    {
      "epoch": 0.44081632653061226,
      "grad_norm": 0.4087633192539215,
      "learning_rate": 0.00017074497009244153,
      "loss": 1.7817,
      "step": 270
    },
    {
      "epoch": 0.45714285714285713,
      "grad_norm": 0.3711621165275574,
      "learning_rate": 0.00016965742251223492,
      "loss": 1.7706,
      "step": 280
    },
    {
      "epoch": 0.47346938775510206,
      "grad_norm": 0.34205716848373413,
      "learning_rate": 0.00016856987493202828,
      "loss": 1.7466,
      "step": 290
    },
    {
      "epoch": 0.4897959183673469,
      "grad_norm": 0.3816305994987488,
      "learning_rate": 0.00016748232735182167,
      "loss": 1.7623,
      "step": 300
    },
    {
      "epoch": 0.5061224489795918,
      "grad_norm": 0.2984622120857239,
      "learning_rate": 0.00016639477977161503,
      "loss": 1.7816,
      "step": 310
    },
    {
      "epoch": 0.5224489795918368,
      "grad_norm": 0.33755743503570557,
      "learning_rate": 0.0001653072321914084,
      "loss": 1.73,
      "step": 320
    },
    {
      "epoch": 0.5387755102040817,
      "grad_norm": 0.28599584102630615,
      "learning_rate": 0.00016421968461120175,
      "loss": 1.7403,
      "step": 330
    },
    {
      "epoch": 0.5551020408163265,
      "grad_norm": 0.3225797712802887,
      "learning_rate": 0.00016313213703099514,
      "loss": 1.8141,
      "step": 340
    },
    {
      "epoch": 0.5714285714285714,
      "grad_norm": 0.34483781456947327,
      "learning_rate": 0.0001620445894507885,
      "loss": 1.7454,
      "step": 350
    },
    {
      "epoch": 0.5877551020408164,
      "grad_norm": 0.3487129211425781,
      "learning_rate": 0.00016095704187058186,
      "loss": 1.7941,
      "step": 360
    },
    {
      "epoch": 0.6040816326530613,
      "grad_norm": 0.33115166425704956,
      "learning_rate": 0.00015986949429037522,
      "loss": 1.8038,
      "step": 370
    },
    {
      "epoch": 0.6204081632653061,
      "grad_norm": 0.3212842643260956,
      "learning_rate": 0.00015878194671016858,
      "loss": 1.7753,
      "step": 380
    },
    {
      "epoch": 0.636734693877551,
      "grad_norm": 0.33445823192596436,
      "learning_rate": 0.00015769439912996194,
      "loss": 1.7847,
      "step": 390
    },
    {
      "epoch": 0.6530612244897959,
      "grad_norm": 0.3554426431655884,
      "learning_rate": 0.00015660685154975532,
      "loss": 1.747,
      "step": 400
    },
    {
      "epoch": 0.6693877551020408,
      "grad_norm": 0.391044557094574,
      "learning_rate": 0.00015551930396954868,
      "loss": 1.7959,
      "step": 410
    },
    {
      "epoch": 0.6857142857142857,
      "grad_norm": 0.3506777882575989,
      "learning_rate": 0.00015443175638934204,
      "loss": 1.7358,
      "step": 420
    },
    {
      "epoch": 0.7020408163265306,
      "grad_norm": 0.3637069761753082,
      "learning_rate": 0.0001533442088091354,
      "loss": 1.7738,
      "step": 430
    },
    {
      "epoch": 0.7183673469387755,
      "grad_norm": 0.32512950897216797,
      "learning_rate": 0.00015225666122892876,
      "loss": 1.7895,
      "step": 440
    },
    {
      "epoch": 0.7346938775510204,
      "grad_norm": 0.30562251806259155,
      "learning_rate": 0.00015116911364872215,
      "loss": 1.7917,
      "step": 450
    },
    {
      "epoch": 0.7510204081632653,
      "grad_norm": 0.31879860162734985,
      "learning_rate": 0.0001500815660685155,
      "loss": 1.7348,
      "step": 460
    },
    {
      "epoch": 0.7673469387755102,
      "grad_norm": 0.3439510464668274,
      "learning_rate": 0.00014899401848830887,
      "loss": 1.7827,
      "step": 470
    },
    {
      "epoch": 0.7836734693877551,
      "grad_norm": 0.37851470708847046,
      "learning_rate": 0.00014790647090810223,
      "loss": 1.7644,
      "step": 480
    },
    {
      "epoch": 0.8,
      "grad_norm": 0.35986047983169556,
      "learning_rate": 0.0001468189233278956,
      "loss": 1.7609,
      "step": 490
    },
    {
      "epoch": 0.8163265306122449,
      "grad_norm": 0.3248812258243561,
      "learning_rate": 0.00014573137574768895,
      "loss": 1.7945,
      "step": 500
    },
    {
      "epoch": 0.8326530612244898,
      "grad_norm": 0.39814943075180054,
      "learning_rate": 0.00014464382816748234,
      "loss": 1.8043,
      "step": 510
    },
    {
      "epoch": 0.8489795918367347,
      "grad_norm": 0.3865431845188141,
      "learning_rate": 0.0001435562805872757,
      "loss": 1.7499,
      "step": 520
    },
    {
      "epoch": 0.8653061224489796,
      "grad_norm": 0.3350381553173065,
      "learning_rate": 0.00014246873300706906,
      "loss": 1.7892,
      "step": 530
    },
    {
      "epoch": 0.8816326530612245,
      "grad_norm": 0.32461676001548767,
      "learning_rate": 0.00014138118542686242,
      "loss": 1.8313,
      "step": 540
    },
    {
      "epoch": 0.8979591836734694,
      "grad_norm": 0.30304235219955444,
      "learning_rate": 0.0001402936378466558,
      "loss": 1.7462,
      "step": 550
    },
    {
      "epoch": 0.9142857142857143,
      "grad_norm": 0.31604698300361633,
      "learning_rate": 0.00013920609026644916,
      "loss": 1.8396,
      "step": 560
    },
    {
      "epoch": 0.9306122448979591,
      "grad_norm": 0.35910564661026,
      "learning_rate": 0.00013811854268624252,
      "loss": 1.7799,
      "step": 570
    },
    {
      "epoch": 0.9469387755102041,
      "grad_norm": 0.37859055399894714,
      "learning_rate": 0.0001370309951060359,
      "loss": 1.7635,
      "step": 580
    },
    {
      "epoch": 0.963265306122449,
      "grad_norm": 0.35883405804634094,
      "learning_rate": 0.00013594344752582927,
      "loss": 1.739,
      "step": 590
    },
    {
      "epoch": 0.9795918367346939,
      "grad_norm": 0.3620239198207855,
      "learning_rate": 0.00013485589994562263,
      "loss": 1.64,
      "step": 600
    },
    {
      "epoch": 0.9959183673469387,
      "grad_norm": 0.3633042871952057,
      "learning_rate": 0.000133768352365416,
      "loss": 1.7418,
      "step": 610
    },
    {
      "epoch": 1.0114285714285713,
      "grad_norm": 0.30214953422546387,
      "learning_rate": 0.00013268080478520938,
      "loss": 1.7592,
      "step": 620
    },
    {
      "epoch": 1.0277551020408162,
      "grad_norm": 0.34190449118614197,
      "learning_rate": 0.00013159325720500274,
      "loss": 1.7388,
      "step": 630
    },
    {
      "epoch": 1.0440816326530613,
      "grad_norm": 0.3426276743412018,
      "learning_rate": 0.0001305057096247961,
      "loss": 1.6961,
      "step": 640
    },
    {
      "epoch": 1.0604081632653062,
      "grad_norm": 0.35828086733818054,
      "learning_rate": 0.00012941816204458946,
      "loss": 1.681,
      "step": 650
    },
    {
      "epoch": 1.076734693877551,
      "grad_norm": 0.3839339017868042,
      "learning_rate": 0.00012833061446438282,
      "loss": 1.7052,
      "step": 660
    },
    {
      "epoch": 1.093061224489796,
      "grad_norm": 0.41961991786956787,
      "learning_rate": 0.0001272430668841762,
      "loss": 1.7308,
      "step": 670
    },
    {
      "epoch": 1.1093877551020408,
      "grad_norm": 0.3190782070159912,
      "learning_rate": 0.00012615551930396956,
      "loss": 1.7539,
      "step": 680
    },
    {
      "epoch": 1.1257142857142857,
      "grad_norm": 0.39733657240867615,
      "learning_rate": 0.00012506797172376292,
      "loss": 1.808,
      "step": 690
    },
    {
      "epoch": 1.1420408163265305,
      "grad_norm": 0.419784277677536,
      "learning_rate": 0.00012398042414355628,
      "loss": 1.7209,
      "step": 700
    },
    {
      "epoch": 1.1583673469387756,
      "grad_norm": 0.4370533525943756,
      "learning_rate": 0.00012289287656334964,
      "loss": 1.7237,
      "step": 710
    },
    {
      "epoch": 1.1746938775510205,
      "grad_norm": 0.3911590874195099,
      "learning_rate": 0.000121805328983143,
      "loss": 1.6792,
      "step": 720
    },
    {
      "epoch": 1.1910204081632654,
      "grad_norm": 0.4202093183994293,
      "learning_rate": 0.00012071778140293639,
      "loss": 1.7578,
      "step": 730
    },
    {
      "epoch": 1.2073469387755102,
      "grad_norm": 0.4403725266456604,
      "learning_rate": 0.00011963023382272975,
      "loss": 1.6709,
      "step": 740
    },
    {
      "epoch": 1.2236734693877551,
      "grad_norm": 0.4282209277153015,
      "learning_rate": 0.00011854268624252311,
      "loss": 1.7659,
      "step": 750
    },
    {
      "epoch": 1.24,
      "grad_norm": 0.40182796120643616,
      "learning_rate": 0.00011745513866231647,
      "loss": 1.6835,
      "step": 760
    },
    {
      "epoch": 1.2563265306122449,
      "grad_norm": 0.42794710397720337,
      "learning_rate": 0.00011636759108210984,
      "loss": 1.6941,
      "step": 770
    },
    {
      "epoch": 1.2726530612244897,
      "grad_norm": 0.4272332489490509,
      "learning_rate": 0.00011528004350190322,
      "loss": 1.7426,
      "step": 780
    },
    {
      "epoch": 1.2889795918367346,
      "grad_norm": 0.4609212875366211,
      "learning_rate": 0.00011419249592169658,
      "loss": 1.7324,
      "step": 790
    },
    {
      "epoch": 1.3053061224489797,
      "grad_norm": 0.48161691427230835,
      "learning_rate": 0.00011310494834148995,
      "loss": 1.7568,
      "step": 800
    },
    {
      "epoch": 1.3216326530612246,
      "grad_norm": 0.3699030876159668,
      "learning_rate": 0.00011201740076128331,
      "loss": 1.6905,
      "step": 810
    },
    {
      "epoch": 1.3379591836734694,
      "grad_norm": 0.41153985261917114,
      "learning_rate": 0.00011092985318107667,
      "loss": 1.7356,
      "step": 820
    },
    {
      "epoch": 1.3542857142857143,
      "grad_norm": 0.4247668981552124,
      "learning_rate": 0.00010984230560087003,
      "loss": 1.7839,
      "step": 830
    },
    {
      "epoch": 1.3706122448979592,
      "grad_norm": 0.4074098467826843,
      "learning_rate": 0.00010875475802066342,
      "loss": 1.7578,
      "step": 840
    },
    {
      "epoch": 1.386938775510204,
      "grad_norm": 0.39793357253074646,
      "learning_rate": 0.00010766721044045678,
      "loss": 1.7218,
      "step": 850
    },
    {
      "epoch": 1.403265306122449,
      "grad_norm": 0.44854822754859924,
      "learning_rate": 0.00010657966286025014,
      "loss": 1.7176,
      "step": 860
    },
    {
      "epoch": 1.4195918367346938,
      "grad_norm": 0.3655751645565033,
      "learning_rate": 0.0001054921152800435,
      "loss": 1.7131,
      "step": 870
    },
    {
      "epoch": 1.4359183673469387,
      "grad_norm": 0.44687238335609436,
      "learning_rate": 0.00010440456769983686,
      "loss": 1.7208,
      "step": 880
    },
    {
      "epoch": 1.4522448979591838,
      "grad_norm": 0.4253818392753601,
      "learning_rate": 0.00010331702011963025,
      "loss": 1.7614,
      "step": 890
    },
    {
      "epoch": 1.4685714285714286,
      "grad_norm": 0.44327694177627563,
      "learning_rate": 0.0001022294725394236,
      "loss": 1.7722,
      "step": 900
    },
    {
      "epoch": 1.4848979591836735,
      "grad_norm": 0.4031260013580322,
      "learning_rate": 0.00010114192495921697,
      "loss": 1.7173,
      "step": 910
    },
    {
      "epoch": 1.5012244897959184,
      "grad_norm": 0.4226587116718292,
      "learning_rate": 0.00010005437737901034,
      "loss": 1.794,
      "step": 920
    },
    {
      "epoch": 1.5175510204081633,
      "grad_norm": 0.5013386011123657,
      "learning_rate": 9.896682979880371e-05,
      "loss": 1.7564,
      "step": 930
    },
    {
      "epoch": 1.5338775510204081,
      "grad_norm": 0.40163445472717285,
      "learning_rate": 9.787928221859707e-05,
      "loss": 1.7359,
      "step": 940
    },
    {
      "epoch": 1.550204081632653,
      "grad_norm": 0.4464458227157593,
      "learning_rate": 9.679173463839043e-05,
      "loss": 1.7888,
      "step": 950
    },
    {
      "epoch": 1.566530612244898,
      "grad_norm": 0.428595632314682,
      "learning_rate": 9.57041870581838e-05,
      "loss": 1.7221,
      "step": 960
    },
    {
      "epoch": 1.5828571428571427,
      "grad_norm": 0.46001940965652466,
      "learning_rate": 9.461663947797717e-05,
      "loss": 1.7238,
      "step": 970
    },
    {
      "epoch": 1.5991836734693878,
      "grad_norm": 0.4248057007789612,
      "learning_rate": 9.352909189777053e-05,
      "loss": 1.7003,
      "step": 980
    },
    {
      "epoch": 1.6155102040816327,
      "grad_norm": 0.3950120210647583,
      "learning_rate": 9.24415443175639e-05,
      "loss": 1.6982,
      "step": 990
    },
    {
      "epoch": 1.6318367346938776,
      "grad_norm": 0.4902910590171814,
      "learning_rate": 9.135399673735726e-05,
      "loss": 1.7592,
      "step": 1000
    },
    {
      "epoch": 1.6481632653061224,
      "grad_norm": 0.4568723440170288,
      "learning_rate": 9.026644915715063e-05,
      "loss": 1.7245,
      "step": 1010
    },
    {
      "epoch": 1.6644897959183673,
      "grad_norm": 0.4254741370677948,
      "learning_rate": 8.9178901576944e-05,
      "loss": 1.7524,
      "step": 1020
    },
    {
      "epoch": 1.6808163265306122,
      "grad_norm": 0.4702874720096588,
      "learning_rate": 8.809135399673735e-05,
      "loss": 1.7786,
      "step": 1030
    },
    {
      "epoch": 1.697142857142857,
      "grad_norm": 0.5394619703292847,
      "learning_rate": 8.700380641653073e-05,
      "loss": 1.7316,
      "step": 1040
    },
    {
      "epoch": 1.7134693877551022,
      "grad_norm": 0.427219420671463,
      "learning_rate": 8.591625883632409e-05,
      "loss": 1.6992,
      "step": 1050
    },
    {
      "epoch": 1.7297959183673468,
      "grad_norm": 0.4236909747123718,
      "learning_rate": 8.482871125611746e-05,
      "loss": 1.7147,
      "step": 1060
    },
    {
      "epoch": 1.746122448979592,
      "grad_norm": 0.5120951533317566,
      "learning_rate": 8.374116367591083e-05,
      "loss": 1.7485,
      "step": 1070
    },
    {
      "epoch": 1.7624489795918368,
      "grad_norm": 0.6120485663414001,
      "learning_rate": 8.26536160957042e-05,
      "loss": 1.7381,
      "step": 1080
    },
    {
      "epoch": 1.7787755102040816,
      "grad_norm": 0.49284103512763977,
      "learning_rate": 8.156606851549757e-05,
      "loss": 1.6919,
      "step": 1090
    },
    {
      "epoch": 1.7951020408163265,
      "grad_norm": 0.44560450315475464,
      "learning_rate": 8.047852093529093e-05,
      "loss": 1.7503,
      "step": 1100
    },
    {
      "epoch": 1.8114285714285714,
      "grad_norm": 0.41311612725257874,
      "learning_rate": 7.939097335508429e-05,
      "loss": 1.7154,
      "step": 1110
    },
    {
      "epoch": 1.8277551020408165,
      "grad_norm": 0.4123150706291199,
      "learning_rate": 7.830342577487766e-05,
      "loss": 1.7285,
      "step": 1120
    },
    {
      "epoch": 1.8440816326530611,
      "grad_norm": 0.45607423782348633,
      "learning_rate": 7.721587819467102e-05,
      "loss": 1.7791,
      "step": 1130
    },
    {
      "epoch": 1.8604081632653062,
      "grad_norm": 0.48414817452430725,
      "learning_rate": 7.612833061446438e-05,
      "loss": 1.7352,
      "step": 1140
    },
    {
      "epoch": 1.8767346938775509,
      "grad_norm": 0.46931901574134827,
      "learning_rate": 7.504078303425775e-05,
      "loss": 1.6841,
      "step": 1150
    },
    {
      "epoch": 1.893061224489796,
      "grad_norm": 0.4224781095981598,
      "learning_rate": 7.395323545405111e-05,
      "loss": 1.7418,
      "step": 1160
    },
    {
      "epoch": 1.9093877551020408,
      "grad_norm": 0.48125138878822327,
      "learning_rate": 7.286568787384447e-05,
      "loss": 1.7634,
      "step": 1170
    },
    {
      "epoch": 1.9257142857142857,
      "grad_norm": 0.4151080250740051,
      "learning_rate": 7.177814029363785e-05,
      "loss": 1.6597,
      "step": 1180
    },
    {
      "epoch": 1.9420408163265306,
      "grad_norm": 0.4481138586997986,
      "learning_rate": 7.069059271343121e-05,
      "loss": 1.712,
      "step": 1190
    },
    {
      "epoch": 1.9583673469387755,
      "grad_norm": 0.5201626420021057,
      "learning_rate": 6.960304513322458e-05,
      "loss": 1.7081,
      "step": 1200
    },
    {
      "epoch": 1.9746938775510205,
      "grad_norm": 0.44257792830467224,
      "learning_rate": 6.851549755301796e-05,
      "loss": 1.7388,
      "step": 1210
    },
    {
      "epoch": 1.9910204081632652,
      "grad_norm": 0.4651468098163605,
      "learning_rate": 6.742794997281132e-05,
      "loss": 1.7875,
      "step": 1220
    },
    {
      "epoch": 2.006530612244898,
      "grad_norm": 0.5512328147888184,
      "learning_rate": 6.634040239260469e-05,
      "loss": 1.7421,
      "step": 1230
    },
    {
      "epoch": 2.0228571428571427,
      "grad_norm": 0.4292203485965729,
      "learning_rate": 6.525285481239805e-05,
      "loss": 1.6948,
      "step": 1240
    },
    {
      "epoch": 2.039183673469388,
      "grad_norm": 0.44928082823753357,
      "learning_rate": 6.416530723219141e-05,
      "loss": 1.6411,
      "step": 1250
    },
    {
      "epoch": 2.0555102040816324,
      "grad_norm": 0.4469171464443207,
      "learning_rate": 6.307775965198478e-05,
      "loss": 1.69,
      "step": 1260
    },
    {
      "epoch": 2.0718367346938775,
      "grad_norm": 0.4810261130332947,
      "learning_rate": 6.199021207177814e-05,
      "loss": 1.7252,
      "step": 1270
    },
    {
      "epoch": 2.0881632653061226,
      "grad_norm": 0.4494698941707611,
      "learning_rate": 6.09026644915715e-05,
      "loss": 1.6477,
      "step": 1280
    },
    {
      "epoch": 2.1044897959183673,
      "grad_norm": 0.506398618221283,
      "learning_rate": 5.9815116911364876e-05,
      "loss": 1.687,
      "step": 1290
    },
    {
      "epoch": 2.1208163265306124,
      "grad_norm": 0.4773637354373932,
      "learning_rate": 5.8727569331158236e-05,
      "loss": 1.6192,
      "step": 1300
    },
    {
      "epoch": 2.137142857142857,
      "grad_norm": 0.5057008862495422,
      "learning_rate": 5.764002175095161e-05,
      "loss": 1.7607,
      "step": 1310
    },
    {
      "epoch": 2.153469387755102,
      "grad_norm": 0.5343075394630432,
      "learning_rate": 5.6552474170744976e-05,
      "loss": 1.6661,
      "step": 1320
    },
    {
      "epoch": 2.1697959183673468,
      "grad_norm": 0.49043041467666626,
      "learning_rate": 5.5464926590538336e-05,
      "loss": 1.6499,
      "step": 1330
    },
    {
      "epoch": 2.186122448979592,
      "grad_norm": 0.46736228466033936,
      "learning_rate": 5.437737901033171e-05,
      "loss": 1.7068,
      "step": 1340
    },
    {
      "epoch": 2.202448979591837,
      "grad_norm": 0.4612702429294586,
      "learning_rate": 5.328983143012507e-05,
      "loss": 1.6503,
      "step": 1350
    },
    {
      "epoch": 2.2187755102040816,
      "grad_norm": 0.475404292345047,
      "learning_rate": 5.220228384991843e-05,
      "loss": 1.6917,
      "step": 1360
    },
    {
      "epoch": 2.2351020408163267,
      "grad_norm": 0.46604084968566895,
      "learning_rate": 5.11147362697118e-05,
      "loss": 1.6936,
      "step": 1370
    },
    {
      "epoch": 2.2514285714285713,
      "grad_norm": 0.47548580169677734,
      "learning_rate": 5.002718868950517e-05,
      "loss": 1.6935,
      "step": 1380
    },
    {
      "epoch": 2.2677551020408164,
      "grad_norm": 0.5979737639427185,
      "learning_rate": 4.8939641109298537e-05,
      "loss": 1.6653,
      "step": 1390
    },
    {
      "epoch": 2.284081632653061,
      "grad_norm": 0.5647278428077698,
      "learning_rate": 4.78520935290919e-05,
      "loss": 1.6549,
      "step": 1400
    },
    {
      "epoch": 2.300408163265306,
      "grad_norm": 0.57192063331604,
      "learning_rate": 4.676454594888526e-05,
      "loss": 1.6899,
      "step": 1410
    },
    {
      "epoch": 2.3167346938775513,
      "grad_norm": 0.5316405296325684,
      "learning_rate": 4.567699836867863e-05,
      "loss": 1.7445,
      "step": 1420
    },
    {
      "epoch": 2.333061224489796,
      "grad_norm": 0.5790471434593201,
      "learning_rate": 4.4589450788472e-05,
      "loss": 1.682,
      "step": 1430
    },
    {
      "epoch": 2.349387755102041,
      "grad_norm": 0.5687270164489746,
      "learning_rate": 4.3501903208265364e-05,
      "loss": 1.7252,
      "step": 1440
    },
    {
      "epoch": 2.3657142857142857,
      "grad_norm": 0.5631235241889954,
      "learning_rate": 4.241435562805873e-05,
      "loss": 1.6671,
      "step": 1450
    },
    {
      "epoch": 2.3820408163265308,
      "grad_norm": 0.5176239013671875,
      "learning_rate": 4.13268080478521e-05,
      "loss": 1.7218,
      "step": 1460
    },
    {
      "epoch": 2.3983673469387754,
      "grad_norm": 0.4787502586841583,
      "learning_rate": 4.0239260467645464e-05,
      "loss": 1.661,
      "step": 1470
    },
    {
      "epoch": 2.4146938775510205,
      "grad_norm": 0.556423008441925,
      "learning_rate": 3.915171288743883e-05,
      "loss": 1.6792,
      "step": 1480
    },
    {
      "epoch": 2.431020408163265,
      "grad_norm": 0.5809676051139832,
      "learning_rate": 3.806416530723219e-05,
      "loss": 1.6697,
      "step": 1490
    },
    {
      "epoch": 2.4473469387755102,
      "grad_norm": 0.5177850723266602,
      "learning_rate": 3.697661772702556e-05,
      "loss": 1.6365,
      "step": 1500
    },
    {
      "epoch": 2.4636734693877553,
      "grad_norm": 0.6393157839775085,
      "learning_rate": 3.5889070146818924e-05,
      "loss": 1.7049,
      "step": 1510
    },
    {
      "epoch": 2.48,
      "grad_norm": 0.5502890944480896,
      "learning_rate": 3.480152256661229e-05,
      "loss": 1.6964,
      "step": 1520
    },
    {
      "epoch": 2.496326530612245,
      "grad_norm": 0.49498042464256287,
      "learning_rate": 3.371397498640566e-05,
      "loss": 1.6833,
      "step": 1530
    },
    {
      "epoch": 2.5126530612244897,
      "grad_norm": 0.5500670075416565,
      "learning_rate": 3.2626427406199024e-05,
      "loss": 1.7108,
      "step": 1540
    },
    {
      "epoch": 2.528979591836735,
      "grad_norm": 0.5249618887901306,
      "learning_rate": 3.153887982599239e-05,
      "loss": 1.6733,
      "step": 1550
    },
    {
      "epoch": 2.5453061224489795,
      "grad_norm": 0.4982493817806244,
      "learning_rate": 3.045133224578575e-05,
      "loss": 1.7043,
      "step": 1560
    },
    {
      "epoch": 2.5616326530612246,
      "grad_norm": 0.590225875377655,
      "learning_rate": 2.9363784665579118e-05,
      "loss": 1.6736,
      "step": 1570
    },
    {
      "epoch": 2.577959183673469,
      "grad_norm": 0.6564599275588989,
      "learning_rate": 2.8276237085372488e-05,
      "loss": 1.7307,
      "step": 1580
    },
    {
      "epoch": 2.5942857142857143,
      "grad_norm": 0.5481823086738586,
      "learning_rate": 2.7188689505165855e-05,
      "loss": 1.7076,
      "step": 1590
    },
    {
      "epoch": 2.6106122448979594,
      "grad_norm": 0.5779063105583191,
      "learning_rate": 2.6101141924959215e-05,
      "loss": 1.6956,
      "step": 1600
    },
    {
      "epoch": 2.626938775510204,
      "grad_norm": 0.5354357957839966,
      "learning_rate": 2.5013594344752585e-05,
      "loss": 1.7225,
      "step": 1610
    },
    {
      "epoch": 2.643265306122449,
      "grad_norm": 0.6101082563400269,
      "learning_rate": 2.392604676454595e-05,
      "loss": 1.652,
      "step": 1620
    },
    {
      "epoch": 2.659591836734694,
      "grad_norm": 0.537060022354126,
      "learning_rate": 2.2838499184339315e-05,
      "loss": 1.6732,
      "step": 1630
    },
    {
      "epoch": 2.675918367346939,
      "grad_norm": 0.5881801247596741,
      "learning_rate": 2.1750951604132682e-05,
      "loss": 1.6872,
      "step": 1640
    },
    {
      "epoch": 2.6922448979591835,
      "grad_norm": 0.5041810870170593,
      "learning_rate": 2.066340402392605e-05,
      "loss": 1.6829,
      "step": 1650
    },
    {
      "epoch": 2.7085714285714286,
      "grad_norm": 0.538841724395752,
      "learning_rate": 1.9575856443719415e-05,
      "loss": 1.6761,
      "step": 1660
    },
    {
      "epoch": 2.7248979591836733,
      "grad_norm": 0.6100420355796814,
      "learning_rate": 1.848830886351278e-05,
      "loss": 1.6596,
      "step": 1670
    },
    {
      "epoch": 2.7412244897959184,
      "grad_norm": 0.5662341117858887,
      "learning_rate": 1.7400761283306145e-05,
      "loss": 1.7311,
      "step": 1680
    },
    {
      "epoch": 2.7575510204081635,
      "grad_norm": 0.6112314462661743,
      "learning_rate": 1.6313213703099512e-05,
      "loss": 1.6938,
      "step": 1690
    },
    {
      "epoch": 2.773877551020408,
      "grad_norm": 0.567021369934082,
      "learning_rate": 1.5225666122892876e-05,
      "loss": 1.8033,
      "step": 1700
    },
    {
      "epoch": 2.790204081632653,
      "grad_norm": 0.5438692569732666,
      "learning_rate": 1.4138118542686244e-05,
      "loss": 1.7428,
      "step": 1710
    },
    {
      "epoch": 2.806530612244898,
      "grad_norm": 0.5912806987762451,
      "learning_rate": 1.3050570962479607e-05,
      "loss": 1.6919,
      "step": 1720
    },
    {
      "epoch": 2.822857142857143,
      "grad_norm": 0.5569177269935608,
      "learning_rate": 1.1963023382272976e-05,
      "loss": 1.7364,
      "step": 1730
    },
    {
      "epoch": 2.8391836734693876,
      "grad_norm": 0.5053794384002686,
      "learning_rate": 1.0875475802066341e-05,
      "loss": 1.7057,
      "step": 1740
    },
    {
      "epoch": 2.8555102040816327,
      "grad_norm": 0.5560097098350525,
      "learning_rate": 9.787928221859708e-06,
      "loss": 1.6653,
      "step": 1750
    },
    {
      "epoch": 2.8718367346938773,
      "grad_norm": 0.549549400806427,
      "learning_rate": 8.700380641653073e-06,
      "loss": 1.6669,
      "step": 1760
    },
    {
      "epoch": 2.8881632653061224,
      "grad_norm": 0.543813169002533,
      "learning_rate": 7.612833061446438e-06,
      "loss": 1.6609,
      "step": 1770
    },
    {
      "epoch": 2.9044897959183675,
      "grad_norm": 0.6006941795349121,
      "learning_rate": 6.525285481239804e-06,
      "loss": 1.7035,
      "step": 1780
    },
    {
      "epoch": 2.920816326530612,
      "grad_norm": 0.5786960124969482,
      "learning_rate": 5.4377379010331704e-06,
      "loss": 1.6468,
      "step": 1790
    },
    {
      "epoch": 2.9371428571428573,
      "grad_norm": 0.5112045407295227,
      "learning_rate": 4.350190320826536e-06,
      "loss": 1.6524,
      "step": 1800
    },
    {
      "epoch": 2.953469387755102,
      "grad_norm": 0.5758779048919678,
      "learning_rate": 3.262642740619902e-06,
      "loss": 1.7382,
      "step": 1810
    },
    {
      "epoch": 2.969795918367347,
      "grad_norm": 0.6262326836585999,
      "learning_rate": 2.175095160413268e-06,
      "loss": 1.6989,
      "step": 1820
    },
    {
      "epoch": 2.9861224489795917,
      "grad_norm": 0.5474936366081238,
      "learning_rate": 1.087547580206634e-06,
      "loss": 1.6819,
      "step": 1830
    }
  ],
  "logging_steps": 10,
  "max_steps": 1839,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 5.865956450500608e+17,
  "train_batch_size": 2,
  "trial_name": null,
  "trial_params": null
}
