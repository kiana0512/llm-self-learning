# train/train_lora.py

import os
from datasets import load_from_disk
from transformers import (
    AutoTokenizer, AutoModelForCausalLM,
    TrainingArguments, Trainer, DataCollatorForLanguageModeling
)
from peft import LoraConfig, get_peft_model, TaskType, prepare_model_for_kbit_training
from transformers import BitsAndBytesConfig

def main():
    # âœ… æ¨¡å‹ä¸æ•°æ®è·¯å¾„é…ç½®
    model_name = "deepseek-ai/deepseek-llm-7b-base"
    dataset_path = "../data/tokenized_wmt19_zh_en"
    output_dir = "./lora_output"

    # âœ… åŠ è½½åˆ†è¯æ•°æ®é›†
    print("ğŸ“‚ åŠ è½½æ•°æ®é›†...")
    dataset = load_from_disk(dataset_path)

    # âœ… åŠ è½½ tokenizer
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)

    # âœ… è®¾ç½® 4bit é‡åŒ–é…ç½®ï¼ˆä¸å†ä¼  load_in_4bitï¼‰
    quant_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_compute_dtype="float16",
        bnb_4bit_use_double_quant=True,
        bnb_4bit_quant_type="nf4"
    )

    # âœ… åŠ è½½æ¨¡å‹ï¼ˆä½¿ç”¨é‡åŒ–é…ç½®ï¼‰
    print("ğŸ“¦ åŠ è½½æ¨¡å‹ä¸­...")
    model = AutoModelForCausalLM.from_pretrained(
        model_name,
        quantization_config=quant_config,
        device_map="auto",
        trust_remote_code=True
    )

    # âœ… åº”ç”¨ LoRA å¾®è°ƒæ¨¡å—
    print("ğŸ”§ åº”ç”¨ LoRA...")
    model.gradient_checkpointing_enable()
    model = prepare_model_for_kbit_training(model)

    lora_config = LoraConfig(
        r=8,
        lora_alpha=16,
        target_modules=["q_proj", "v_proj"],
        lora_dropout=0.05,
        bias="none",
        task_type=TaskType.CAUSAL_LM
    )
    model = get_peft_model(model, lora_config)

    # âœ… æ•°æ®é›†åˆ‡åˆ†ï¼ˆ98% è®­ç»ƒ + 2% éªŒè¯ï¼‰
    dataset = dataset.shuffle(seed=42)
    split = dataset.train_test_split(test_size=0.02)
    train_data = split["train"]
    eval_data = split["test"]

    # âœ… è®­ç»ƒå‚æ•°è®¾ç½®
    training_args = TrainingArguments(
        output_dir=output_dir,
        per_device_train_batch_size=2,
        gradient_accumulation_steps=8,
        num_train_epochs=3,
        learning_rate=2e-4,
        bf16=True,  # è‹¥ä¸æ”¯æŒï¼Œå¯æ”¹ä¸º fp16=True
        logging_steps=10,
        save_strategy="epoch",
        evaluation_strategy="epoch",
        save_total_limit=2,
        report_to="none"
    )

    # âœ… è‡ªåŠ¨å¡«å……å™¨ï¼šé€‚é…è¯­è¨€æ¨¡å‹è®­ç»ƒéœ€æ±‚
    data_collator = DataCollatorForLanguageModeling(
        tokenizer=tokenizer,
        mlm=False,
        return_tensors="pt"
    )

    # âœ… æ„å»º Trainer
    print("ğŸš€ å¯åŠ¨è®­ç»ƒ...")
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_data,
        eval_dataset=eval_data,
        data_collator=data_collator,
        tokenizer=tokenizer
    )

    trainer.train()
    trainer.save_model(output_dir)
    print("âœ… è®­ç»ƒå®Œæˆï¼Œæ¨¡å‹ä¿å­˜åœ¨ï¼š", output_dir)

if __name__ == "__main__":
    main()
