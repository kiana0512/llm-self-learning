import os
import shutil
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel, PeftConfig

# Step 1: é…ç½®è·¯å¾„
adapter_path = "./train/lora_output"
output_path = "./deployment/merged_model"
base_model = "deepseek-ai/deepseek-llm-7b-base"

# Step 2: åŠ è½½å¹¶åˆå¹¶æ¨¡å‹
print("ğŸ“¦ æ­£åœ¨åŠ è½½ Base æ¨¡å‹ + LoRA Adapter...")
base = AutoModelForCausalLM.from_pretrained(base_model, torch_dtype="auto", device_map="auto")
model = PeftModel.from_pretrained(base, adapter_path)
model = model.merge_and_unload()

# Step 3: ä¿å­˜åˆå¹¶åçš„æ¨¡å‹
print(f"ğŸ’¾ ä¿å­˜è‡³ï¼š{output_path}")
model.save_pretrained(output_path)
tokenizer = AutoTokenizer.from_pretrained(adapter_path)
tokenizer.save_pretrained(output_path)

# Step 4: å¯é€‰å‹ç¼©
shutil.make_archive(output_path, 'zip', output_path)
print("âœ… æ¨¡å‹ä¿å­˜å¹¶å‹ç¼©å®Œæˆã€‚")
