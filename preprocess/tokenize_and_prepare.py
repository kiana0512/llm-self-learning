# preprocess/tokenize_and_prepare.py

import os
import json
from datasets import Dataset
from transformers import AutoTokenizer

def load_jsonl(filepath):
    """è¯»å– .jsonl æ–‡ä»¶ï¼Œè¿”å›ä¸€ä¸ª list[dict]"""
    data = []
    with open(filepath, 'r', encoding='utf-8') as f:
        for line in f:
            data.append(json.loads(line))
    return data

def tokenize_sample(example, tokenizer, max_length=512):
    """
    å°†å•ä¸ªæ ·æœ¬ä¸­çš„ prompt + response æ‹¼æ¥ä¸ºè¾“å…¥ï¼Œè¿›è¡Œåˆ†è¯ã€‚

    æ³¨æ„ï¼š
    - input_ids å’Œ labels ä¸€æ ·ï¼Œéƒ½æ˜¯ prompt + response çš„ tokenã€‚
    - å¦‚æœå¸Œæœ›åªè®­ç»ƒ responseï¼Œå¯åšé¢å¤– maskingï¼ˆé«˜çº§ç­–ç•¥ï¼‰ã€‚
    """
    text = example["prompt"] + "\n\n" + example["response"]

    # tokenizer.encode_plus è¿”å›åŒ…å« input_idsã€attention_mask ç­‰å­—æ®µ
    tokenized = tokenizer(
        text,
        truncation=True,
        padding="max_length",
        max_length=max_length,
    )

    # æ·»åŠ  labels å­—æ®µï¼ˆè¯­è¨€æ¨¡å‹çš„ç›‘ç£ç›®æ ‡ï¼‰
    tokenized["labels"] = tokenized["input_ids"].copy()
    return tokenized

def prepare_dataset(jsonl_path, tokenizer_name, max_length=512):
    """
    åŠ è½½ jsonl æ•°æ® â†’ åˆ†è¯ â†’ æ„å»º Dataset

    å‚æ•°è¯´æ˜ï¼š
    - jsonl_path: ç¬¬ä¸€æ­¥ä¿å­˜çš„æ•°æ®æ–‡ä»¶è·¯å¾„
    - tokenizer_name: æ¨¡å‹çš„ tokenizer åç§°ï¼ˆå¦‚ deepseek-ai/deepseek-llm-7b-baseï¼‰
    """
    print(f"ğŸ“‚ åŠ è½½æ•°æ®æ–‡ä»¶: {jsonl_path}")
    raw_data = load_jsonl(jsonl_path)

    print(f"ğŸ”„ åŠ è½½ Tokenizerï¼š{tokenizer_name}")
    tokenizer = AutoTokenizer.from_pretrained(tokenizer_name, trust_remote_code=True)

    print("ğŸ§¹ æ‰§è¡Œåˆ†è¯...")
    dataset = Dataset.from_list(raw_data)
    dataset = dataset.map(lambda x: tokenize_sample(x, tokenizer, max_length), remove_columns=["prompt", "response"])

    print("âœ… æ•°æ®é›†æ„å»ºå®Œæˆã€‚æ ·æœ¬æ€»æ•°:", len(dataset))
    return dataset, tokenizer

if __name__ == "__main__":
    # ç¤ºä¾‹è°ƒç”¨
    dataset,tokenizer = prepare_dataset(
        jsonl_path="../data/wmt19_zh_en.jsonl",
        tokenizer_name="deepseek-ai/deepseek-llm-7b-base",  # å¯æ ¹æ®å®é™…ä¿®æ”¹
        max_length=512
    )

    # ç¤ºä¾‹è¾“å‡ºé¦–æ¡æ•°æ®
    print(dataset[0])
    print(tokenizer.decode(dataset[0]["input_ids"], skip_special_tokens=True))

    # âœ… ä¿å­˜æˆç£ç›˜æ•°æ®é›†ä»¥ä¾›è®­ç»ƒå¤ç”¨
    dataset.save_to_disk("../data/tokenized_wmt19_zh_en")
    print("âœ… åˆ†è¯æ•°æ®é›†å·²ä¿å­˜è‡³ ../data/tokenized_wmt19_zh_en")
